{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46edd589-08e6-41d7-9ac5-50bd9e51fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_aspect_ratio(img, new_width=None, new_height=None):\n",
    "    # Get the current height and width\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    # If only width is specified\n",
    "    if new_width is not None and new_height is None:\n",
    "        # Calculate the aspect ratio and new height\n",
    "        aspect_ratio = width / height\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "\n",
    "    # If only height is specified\n",
    "    elif new_height is not None and new_width is None:\n",
    "        # Calculate the aspect ratio and new width\n",
    "        aspect_ratio = height / width\n",
    "        new_width = int(new_height / aspect_ratio)\n",
    "\n",
    "    # If both width and height are specified, ignore aspect ratio\n",
    "    elif new_width is not None and new_height is not None:\n",
    "        pass\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(img, (new_width, new_height))\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6975517-5323-42aa-85c1-b0132c50d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, landmarks):\n",
    "\n",
    "    radius = 5\n",
    "    # Check if image width is greater than 1000 px.\n",
    "    # To improve visualization.\n",
    "    print('*********** landmarks: ', landmarks)\n",
    "    \n",
    "    if (image.shape[1] > 1000):\n",
    "        radius = 8\n",
    "\n",
    "    for idx, kpt_data in enumerate(landmarks):\n",
    "\n",
    "        # loc_x, loc_y = kpt_data[:2].astype(\"int\").tolist()\n",
    "        loc_x, loc_y = kpt_data[:2]\n",
    "        \n",
    "        color_id = list(COLORS_RGB_MAP[int(kpt_data[-1])].values())[0]\n",
    "\n",
    "        cv2.circle(image,\n",
    "                   (loc_x, loc_y),\n",
    "                   radius,\n",
    "                   color=color_id[::-1],\n",
    "                   thickness=-1,\n",
    "                   lineType=cv2.LINE_AA)\n",
    "         # Draw keypoint number\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        font_thickness = 1\n",
    "        # text = str(int(kpt_data[-1]))\n",
    "        text = f\"{int(kpt_data[-1])}: ({loc_x}, {loc_y})\"\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
    "        text_x = loc_x - text_size[0] // 2\n",
    "        text_y = loc_y - radius - 5\n",
    "        print('************* text:', text)\n",
    "        cv2.putText(image, text, (text_x, text_y), font, font_scale, color=(255, 255, 255), thickness=font_thickness)\n",
    "\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "befe72a5-aa9a-48fe-94ff-7caa43ff4670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wear_collar(accessories_img,animal_image,  filter_kpts):\n",
    "    ############################### cap working ############################### \n",
    "    # get value of 8 and 2\n",
    "    point_8 = next(sublist for sublist in filter_kpts if sublist[2] == 8)\n",
    "    point_2 = next(sublist for sublist in filter_kpts if sublist[2] == 2)\n",
    "    print('point_8 and point_2', point_8, point_2 )\n",
    "\n",
    "    ### check if point 8 and point 2 exits \n",
    "    if point_8 and point_2:\n",
    "        print('exits+++++++++++++++++')\n",
    "\n",
    "        #### width between 8 and 2\n",
    "        width_8_2 = np.sqrt((point_2[0] - point_8[0])**2 + (point_2[1] - point_8[1])**2)\n",
    "\n",
    "        #### resize image according to width if the 8 and 2 points \n",
    "        resized_accessories_img  = resize_with_aspect_ratio(accessories_img, int(width_8_2))\n",
    "\n",
    "        # Calculate the midpoint\n",
    "        midpoint_x = int((point_2[0] + point_8[0]) / 2)\n",
    "        midpoint_y = int((point_2[1] + point_8[1]) / 2)\n",
    "        print(\"&&&&&&&& mid between 8 and 2 midpoint_x  and midpoint_y\", midpoint_x, midpoint_y)\n",
    "\n",
    "        \n",
    "        point_17 = next(sublist for sublist in filter_kpts if sublist[2] == 17)\n",
    "        \n",
    "        mid_17_bet_width_8_2 = np.sqrt((midpoint_x - point_17[0])**2 + (midpoint_y - point_17[1])**2)\n",
    "        print('width between mid and 17: ', mid_17_bet_width_8_2)\n",
    "\n",
    "        if mid_17_bet_width_8_2 <= 100:\n",
    "            percentage_to_subtract = 35\n",
    "            offset_y = int(percentage_to_subtract / 100 * resized_accessories_img.shape[1])\n",
    "            image = cvzone.overlayPNG(animal_image, resized_accessories_img, (point_8[0] , point_8[1] - offset_y))\n",
    "            \n",
    "        else:\n",
    "            midpoint_x_17 = int((midpoint_x + point_17[0]) / 2)\n",
    "            midpoint_y_17 = int((midpoint_y + point_17[1]) / 2)\n",
    "            print(\"^^^^^^^^^^^^^^^^ mid between mid and 7 \", midpoint_x_17, midpoint_y_17)\n",
    "            \n",
    "            ######## show point in specific location\n",
    "            percentage_to_subtract = 45\n",
    "            offset_y = int(percentage_to_subtract / 100 * resized_accessories_img.shape[1])\n",
    "    \n",
    "            \n",
    "            \n",
    "            image = cvzone.overlayPNG(animal_image, resized_accessories_img, (midpoint_x_17 - offset_y, midpoint_y_17 - offset_y))\n",
    "        return image        \n",
    "        # cv2.imwrite(\"resized_accessories_img_check.jpg\", image)\n",
    "        # cv2.imshow(\"resized_accessories_img\", image)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "            \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        missing_points = [point for point in points_to_check if point not in [sublist[2] for sublist in filter_kpts]]\n",
    "        print(f\" ************The following points are missing: {missing_points}\")\n",
    "    ############################### cap working end ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "badd7668-0152-4650-901d-a97e46344c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### cap working ############################### \n",
    "def wear_cap(cap,animal_image,  filter_kpts):\n",
    "    \n",
    "\n",
    "    ################# Check two points (if 14 and 15 exist in the third item of each list)\n",
    "    \n",
    "    point_14_exists = any(14 == sublist[2] for sublist in filter_kpts)\n",
    "    point_15_exists = any(15 == sublist[2] for sublist in filter_kpts)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ################ check four points \n",
    "\n",
    "    # Points to check\n",
    "    points_to_check = [14, 15, 19, 18]\n",
    "    \n",
    "    # Check if all points exist in the third item of any list\n",
    "    all_points_exist = all(point in [sublist[2] for sublist in filter_kpts] for point in points_to_check)\n",
    "    print(f'----------****** four points exits ******----------{all_points_exist}', )\n",
    "    if all_points_exist:\n",
    "        print(\"************ All points exist.\")\n",
    "        \n",
    "\n",
    "        ## *********** check if 19 > 15 and 18 > 14 (condition to correct image flip )\n",
    "        # get value of 19 and 15\n",
    "        point_19 = next(sublist for sublist in filter_kpts if sublist[2] == 19)\n",
    "        point_15 = next(sublist for sublist in filter_kpts if sublist[2] == 15)\n",
    "\n",
    "        if point_19[1] > point_15[1]:\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "            # get value of 18 and 14\n",
    "            point_18 = next(sublist for sublist in filter_kpts if sublist[2] == 18)\n",
    "            point_14 = next(sublist for sublist in filter_kpts if sublist[2] == 14)\n",
    "\n",
    "            # Calculate the width between points 14 and 15\n",
    "            width_19_15 = np.sqrt((point_19[0] - point_15[0])**2 + (point_19[1] - point_15[1])**2)\n",
    "            \n",
    "            # Calculate the width between points 14 and 15\n",
    "            width_18_14 = np.sqrt((point_18[0] - point_14[0])**2 + (point_18[1] - point_14[1])**2)\n",
    "            \n",
    "            print('>>>>>>>>>>>>>>>> length of 18 and 14: ', width_18_14)\n",
    "            print('>>>>>>>>>>>>>>>> length of 19 and 15: ', width_19_15)\n",
    "            print('\\n19, 15, 18, 14 origranl', [point_19, point_15, point_18, point_14])\n",
    "    \n",
    "            point_19[1] = int(point_19[1] - 2 * width_19_15)\n",
    "            point_18[1] = int(point_18[1] - 2 * width_18_14)\n",
    "            \n",
    "            print('19, 15, 18, 14 decreamented\\n', [point_19, point_15, point_18, point_14])\n",
    "\n",
    "            filter_kpts[-1] = point_19 \n",
    "            filter_kpts[-2] = point_18\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        # cap = cv2.imread(accessories_img_path, cv2.IMREAD_UNCHANGED)\n",
    "        cap_h,cap_w =  cap.shape[:2]\n",
    "\n",
    "        print('******** new filter_kpts: ', filter_kpts)\n",
    "        \n",
    "        \n",
    "        head_coordinates = [item for item in filter_kpts if item[2] in points_to_check]\n",
    "        head_coordinates_position = sorted(head_coordinates, key=lambda x: x[2], reverse=True)\n",
    "        print('-------------------- head coordinates position with keypoint numbers: ', head_coordinates_position)\n",
    "        print('-------------------- length of head coordinates position: ', len(head_coordinates_position))\n",
    "        \n",
    "        head_coordinates_position = [[item[0], item[1] + inc] for item in head_coordinates_position]\n",
    "        print('-------------------- head coordinates position without keypoint numbers: ', head_coordinates_position)\n",
    "        \n",
    "        pts1=np.float32([[0,0],[cap_w,0],[0,cap_h],[cap_w,cap_h]])\n",
    "        pts2=np.float32(head_coordinates_position)\n",
    "        \n",
    "        h, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC,5.0)\n",
    "    \n",
    "        height, width, channels = animal_image.shape\n",
    "        im1Reg = cv2.warpPerspective(cap, h, (width, height))\n",
    "        # animal_image = cv2.cvtColor(animal_image, cv2.COLOR_BGR2RGB) \n",
    "        animal_image_result = cvzone.overlayPNG(animal_image, im1Reg, (0, 0))\n",
    "        \n",
    "        # cv2.imshow(\"Original Image\", animal_image_result)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        return animal_image_result\n",
    "    \n",
    "    elif point_14_exists and point_15_exists:\n",
    "\n",
    "        \n",
    "        # get value of 14 and 15\n",
    "        point_14 = next(sublist for sublist in filter_kpts if sublist[2] == 14)\n",
    "        point_15 = next(sublist for sublist in filter_kpts if sublist[2] == 15)\n",
    "        \n",
    "        print(\"********************* Both 14 and 15 exist.\")\n",
    "        \n",
    "\n",
    "        \n",
    "        # Calculate the width between points 14 and 15\n",
    "        width_14_15 = np.sqrt((point_14[0] - point_15[0])**2 + (point_14[1] - point_15[1])**2)\n",
    "        resized_accessories_img  = resize_with_aspect_ratio(cap, int(width_14_15))\n",
    "\n",
    "        \n",
    "        print(f\"********************* The width between points 14 and 15 is: {width_14_15}\")\n",
    "        # animal_image = cv2.cvtColor(animal_image, cv2.COLOR_RGB2BGR) \n",
    "\n",
    "        print('%%%%%%%%%%% shape of png', resized_accessories_img.shape)\n",
    "        \n",
    "\n",
    "        percentage_to_subtract = 38 \n",
    "        offset_y = int(percentage_to_subtract / 100 * resized_accessories_img.shape[1])\n",
    "        \n",
    "        animal_image_result = cvzone.overlayPNG(animal_image, resized_accessories_img, (point_15[0], point_15[1] - offset_y ))\n",
    "\n",
    "        \n",
    "        # cv2.imshow(\"Original Image\", animal_image_result)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        return animal_image_result\n",
    "    ################# Check two points code end\n",
    "    else:\n",
    "        missing_points = [point for point in points_to_check if point not in [sublist[2] for sublist in filter_kpts]]\n",
    "        print(f\" ************The following points are missing: {missing_points}\")\n",
    "############################### cap working end ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d29684-ca46-438b-bd14-c5bc6e13f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\closet\\tryon\\animal-tryon\\input\\dog1.jpg: 640x448 1 dog, 1124.7ms\n",
      "Speed: 49.0ms preprocess, 1124.7ms inference, 142.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "results.boxes.xyxy.numpy() [[         89          80         587         926]]\n",
      "******** old filter_kpts:  [[432, 906, 0], [402, 794, 1], [391, 613, 2], [202, 895, 6], [224, 785, 7], [227, 606, 8], [423, 164, 14], [304, 147, 15], [343, 322, 16], [341, 378, 17], [450, 94, 18], [294, 73, 19]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wear_collar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m animal_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(animal_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# animal_image = draw_landmarks(animal_image, filter_kpts)\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m img_result \u001b[38;5;241m=\u001b[39m \u001b[43mwear_collar\u001b[49m(accessories_img, animal_image, filter_kpts)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# cv2.imwrite(result_png, img_result) \u001b[39;00m\n\u001b[0;32m     80\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcap_wear_result\u001b[39m\u001b[38;5;124m\"\u001b[39m, img_result)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wear_collar' is not defined"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvzone\n",
    "from PIL import Image \n",
    "\n",
    "model_path = 'animal-tryon/best.pt'\n",
    "keywords_file = \"animal-tryon/keypoint_definitions.csv\"\n",
    "# animal_img = 'animal-tryon/input/HD-wallpaper-dog-ears-sitting-big.jpg'\n",
    "# animal_img = 'animal-tryon/images_data/frame_56.png'\n",
    "animal_img = 'animal-tryon/input/dog1.jpg'\n",
    "\n",
    "# accessories_img = 'animal-tryon/accessories/collar-edited.png'\n",
    "# result_png = 'png_result.png'\n",
    "accessories_img_path = \"animal-tryon/accessories/edited_images/16 Collra pastel/เต้าหู้-11647.png\"\n",
    "result_png = \"3.3. 16 Collra pastel(Product result).png\"\n",
    "\n",
    "cap_accessories_img_path = \"animal-tryon/accessories/edited_images/07 Sky & Cloud/เหลือง3.png\"\n",
    "\n",
    "\n",
    "ann_meta_data = pd.read_csv(keywords_file)\n",
    "COLORS = ann_meta_data[\"Hex colour\"].values.tolist()\n",
    "\n",
    "COLORS_RGB_MAP = []\n",
    "for color in COLORS:\n",
    "    R, G, B = int(color[:2], 16), int(color[2:4], 16), int(color[4:], 16)\n",
    "    COLORS_RGB_MAP.append({color: (R,G,B)})\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "BOX_IOU_THRESH = 0.55\n",
    "BOX_CONF_THRESH=0.30\n",
    "KPT_CONF_THRESH=0.68\n",
    "\n",
    "inc = 15\n",
    "\n",
    "\n",
    "\n",
    "animal_image = cv2.imread(animal_img)\n",
    "animal_image = cv2.cvtColor(animal_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "accessories_img = cv2.imread(accessories_img_path, cv2.IMREAD_UNCHANGED)\n",
    "cap_accessories_img = cv2.imread(cap_accessories_img_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "results = model.predict(animal_img, conf=BOX_CONF_THRESH, iou=BOX_IOU_THRESH)[0].cpu()\n",
    "\n",
    "if not len(results.boxes.xyxy):\n",
    "    animal_image\n",
    "\n",
    "# Get the predicted boxes, conf scores and keypoints.\n",
    "pred_boxes = results.boxes.xyxy.numpy()\n",
    "pred_box_conf = results.boxes.conf.numpy()\n",
    "pred_kpts_xy = results.keypoints.xy.numpy()\n",
    "pred_kpts_conf = results.keypoints.conf.numpy()\n",
    "\n",
    "print('results.boxes.xyxy.numpy()', results.boxes.xyxy.numpy())\n",
    "\n",
    "\n",
    "\n",
    "# Draw predicted bounding boxes, conf scores and keypoints on image.\n",
    "for boxes, score, kpts, confs in zip(pred_boxes, pred_box_conf, pred_kpts_xy, pred_kpts_conf):\n",
    "    kpts_ids = np.where(confs > KPT_CONF_THRESH)[0]\n",
    "    filter_kpts = kpts[kpts_ids]\n",
    "    filter_kpts = np.concatenate([filter_kpts, np.expand_dims(kpts_ids, axis=-1)], axis=-1)\n",
    "    \n",
    "    # filter_kpts = filter_kpts.astype(\"int\").tolist()\n",
    "    filter_kpts = [[int(x) for x in inner_list] for inner_list in filter_kpts]\n",
    "    print('******** old filter_kpts: ', filter_kpts)\n",
    "    # animal_image = draw_landmarks(animal_image, filter_kpts)\n",
    "\n",
    "    animal_image = cv2.cvtColor(animal_image, cv2.COLOR_BGR2RGB)\n",
    "    # animal_image = draw_landmarks(animal_image, filter_kpts)\n",
    "    \n",
    "    img_result = wear_collar(accessories_img, animal_image, filter_kpts)\n",
    "    # cv2.imwrite(result_png, img_result) \n",
    "    cv2.imshow(\"cap_wear_result\", img_result)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    img_result = wear_cap(cap_accessories_img, img_result, filter_kpts)\n",
    "\n",
    "    \n",
    "    # Check if the resulting image has valid dimensions\n",
    "    if img_result is not None and img_result.shape[0] > 0 and img_result.shape[1] > 0:\n",
    "        # cv2.imwrite(result_png, img_result) \n",
    "        cv2.imshow(\"cap_wear_result\", img_result)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        print(\"Error: Invalid dimensions for the resulting image.\")\n",
    "\n",
    "        # print(\"********************* Neither 14 nor 15 exists.\")\n",
    "    ################ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb519b-c6b5-40dc-90dd-173fbcc91a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a21f13-e663-49d5-8da3-6abbb1c487aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
